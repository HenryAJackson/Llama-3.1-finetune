# Llama-3.1-finetune

Fine-tuning the Llama 3.1 model using custom data to enhance its text summarization capabilities. Utilizes LoRA and 4-bit quantization as well as auto-tokenization. This model is optimized for efficient training on limited GPU resources. The dataset in use uses paired stories and summaries, enabling the model to learn to generate coherent summaries based off of the provided story. This project aims to demonstrate improved performance in generating contextual summaries, tailored to user-defined instructions.
